# -*- coding: utf-8 -*-
"""TheModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L_mQfHN4JKUwQe06i_WMkqISaULwlYMa
"""

!pip install nltk
import nltk
import pandas as pd
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re
from textblob import TextBlob
nltk.download('punkt')

df=pd.read_csv('instagram.csv')

df.colnums

sample_size = 35000
df = df.head(sample_size)

import re
import string
import nltk
from nltk.corpus import stopwords

# Download stopwords if not already downloaded
nltk.download('stopwords')

def cleantext(text):
    # Removing mentions
    text = re.sub(r"@[0-9a-zA-Z]+", "", text)
    # Removing '#' from reviews
    text = re.sub(r"#", "", text)
    # Removing hyperlinks
    text = re.sub(r"https?:\\/\\/\\S+", "", text)
    # Removing punctuation
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)

    # Remove emojis
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F700-\U0001F77F"  # alchemical symbols
                           u"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
                           u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
                           u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
                           u"\U0001FA00-\U0001FA6F"  # Chess Symbols
                           u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
                           u"\U0001FB00-\U0001FBFF"  # Symbols and Pictographs Extended-B
                           u"\U0001FC00-\U0001FCFF"  # Symbols and Pictographs Extended-C
                           u"\U0001FD00-\U0001FDFF"  # Symbols and Pictographs Extended-D
                           u"\U0001FE00-\U0001FEFF"  # Combining Diacritical Marks Extended
                           u"\U0001FF00-\U0001FFFF"  # Greek Extended
                           "]+", flags=re.UNICODE)

    text = emoji_pattern.sub(r'', text)

    # Tokenize the text
    words = text.split()

    # Remove common stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word.lower() not in stop_words]

    # Reconstruct the text
    text = ' '.join(words)

    return text

df[cleaned]=df[review_description].apply(cleantext )

#def calculate_sentiments(ds):
    #sentiments = TextBlob(ds['review_description']).sentiment
    #return pd.Series([sentiments.subjectivity, sentiments.polarity])

#df[['Subjectivity', 'Polarity']] = df.apply(calculate_sentiments, axis = 1)
#df.head()

#def categorize_sentiment(score):
   # if score < 0:
       # return "Negative"
    #elif score == 0:
       # return "Neutral"
    #else:
       # return "Positive"

#df['Sentiment Analysis'] = df['Polarity'].apply(categorize_sentiment)
#df.head(200)

#positive_reviews = df[df['Sentiment Analysis'] == 'Positive']
#negative_reviews = df[df['Sentiment Analysis'] == 'Negative']
#neutral_reviews = df[df['Sentiment Analysis'] == 'Neutral']

#positive_reviews['tokens'] = positive_reviews['review_description'].apply(lambda x: TextBlob(x).words)
#negative_reviews['tokens'] = negative_reviews['review_description'].apply(lambda x: TextBlob(x).words)
#neutral_reviews['tokens'] = neutral_reviews['review_description'].apply(lambda x: TextBlob(x).words)

df

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import csr_matrix

# Split data into features (X) and target (y)
X = df[['Sentiment Analysis', 'review_description']]
y = df['rating']

# Perform one-hot encoding for the 'Sentiment' column
X = pd.concat([X[['review_description']], pd.get_dummies(X['Sentiment Analysis'], prefix='Sentiment Analysis')], axis=1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert text data to TF-IDF features
tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['review_description'])
X_test_tfidf = tfidf_vectorizer.transform(X_test['review_description'])

# Convert sparse TF-IDF matrices to dense arrays
X_train_tfidf = X_train_tfidf.toarray()
X_test_tfidf = X_test_tfidf.toarray()

model = keras.Sequential([
    keras.layers.Input(shape=(X_train_tfidf.shape[1],)),
    keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),  # Added L2 regularization
    keras.layers.Dropout(0.5),
    keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),  # Added L2 regularization
    keras.layers.Dropout(0.5),
    keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),  # Added L2 regularization
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1, activation='linear')
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Define the EarlyStopping callback
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    patience=10,          # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restore the model weights from the epoch with the best validation loss
)

# Train the model with early stopping
history = model.fit(
    X_train_tfidf, y_train,
    epochs=60, batch_size=20,
    validation_split=0.2,
    callbacks=[early_stopping]  # Add the early stopping callback
)

# Evaluate the model on the test set
test_loss = model.evaluate(X_test_tfidf, y_test)
print(f'Test Loss: {test_loss}')



import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Assuming you have already trained your model and have predictions on the test set
# Replace 'y_pred' with your model's predictions
y_pred = model.predict(X_test_tfidf)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)

# Calculate R-squared (R²)
r2 = r2_score(y_test, y_pred)

# Display the results
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"R-squared (R²): {r2:.2f}")

import matplotlib.pyplot as plt

# Your epoch and loss values
epochs = range(1, 41)  # Assuming 40 epochs

# Training and validation loss values for the epochs you provided
training_loss = [
    2.5419, 2.0792, 1.9852, 1.9468, 1.8957, 1.8737, 1.8670, 1.8223, 1.8153, 1.7969,
    1.7957, 1.7970, 1.7824, 1.7890, 1.7872, 1.7919, 1.7802, 1.7808, 1.7758, 1.7649,
    1.7720, 1.7710, 1.7577, 1.7687, 1.7609, 1.7615, 1.7799, 1.7573, 1.7588, 1.7674,
    1.7593, 1.7350, 1.7490, 1.7498, 1.7482, 1.7558, 1.7439, 1.7466, 1.7315, 1.7587
]

validation_loss = [
    1.8771, 1.7982, 1.7892, 1.7735, 1.7505, 1.7249, 1.7588, 1.7788, 1.7005, 1.7417,
    1.7417, 1.6842, 1.6969, 1.7128, 1.6764, 1.6927, 1.7037, 1.6621, 1.6699, 1.6514,
    1.6794, 1.6808, 1.6731, 1.6963, 1.7510, 1.6721, 1.6602, 1.6665, 1.6527, 1.6473,
    1.6476, 1.6494, 1.6669, 1.6598, 1.6594, 1.6724, 1.6590, 1.7009, 1.7127, 1.6896
]

# Ensure both lists have the same number of elements
if len(epochs) != len(training_loss) or len(epochs) != len(validation_loss):
    min_length = min(len(epochs), len(training_loss), len(validation_loss))
    epochs = epochs[:min_length]
    training_loss = training_loss[:min_length]
    validation_loss = validation_loss[:min_length]

# Create a figure and axis for the line chart
plt.figure(figsize=(10, 6))

# Plot training loss
plt.plot(epochs, training_loss, label='Training Loss', marker='o', linestyle='-')

# Plot validation loss
plt.plot(epochs, validation_loss, label='Validation Loss', marker='o', linestyle='-')

# Set labels and title
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss Over Epochs')

# Add a legend
plt.legend()

# Display the plot
plt.grid()
plt.show()